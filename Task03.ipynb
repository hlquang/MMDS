{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZaEtuSZ77lts",
        "5h2rRqQV8Hzz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark Setup"
      ],
      "metadata": {
        "id": "ZaEtuSZ77lts"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1mH4i8r7ecC"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!cp drive/MyDrive/MMDS-data/spark-3.1.1-bin-hadoop3.2.tgz .\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "Y1h1gDd875S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "wuBCwWue76uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('Task3').getOrCreate()"
      ],
      "metadata": {
        "id": "r3wBsKRJ7793"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "L-5D05W57_a9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read data"
      ],
      "metadata": {
        "id": "5h2rRqQV8Hzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = 'ratings2k.csv'"
      ],
      "metadata": {
        "id": "MfbaBs348B88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read \\\n",
        "          .csv(data_path, header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "pEkTmKHP8U_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Required Libraries"
      ],
      "metadata": {
        "id": "_VLQIpLvzIfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import Row\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import SparseVector\n",
        "from pyspark.ml.stat import Correlation\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql import Window\n",
        "import random"
      ],
      "metadata": {
        "id": "y7Bf-Um3Gfpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collaborative Filtering"
      ],
      "metadata": {
        "id": "08-5yeTkzPuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CollaborativeFiltering:\n",
        "  def __init__(self, N, dataframe):\n",
        "    self.N = N # Number of similar users\n",
        "    self.df = dataframe\n",
        "    self.utility_matrix = self.to_utility_matrix(self.df)\n",
        "\n",
        "  def to_utility_matrix(self, dataframe):\n",
        "    # Exclude column 'index'\n",
        "    # Group by 'item' to prepare for pivoting\n",
        "    # Pivot the DataFrame to turn unique users into columns and their ratings into values\n",
        "    # Aggregate to handle multiple ratings from the same user for an item (if any)\n",
        "    # Fill in missing values with 0 to indicate the absence of a rating\n",
        "    um = dataframe.select('user', 'item', 'rating') \\\n",
        "                  .groupBy('item') \\\n",
        "                  .pivot('user') \\\n",
        "                  .agg(F.first('rating')) \\\n",
        "                  .na.fill(0.0) \\\n",
        "                  .sort('item')\n",
        "    return um\n",
        "\n",
        "  def predict(self, user_vector, k):\n",
        "    # Convert SparseVector to create dataframe that the column is that user and rows are their rating for each item\n",
        "    data = [(float(user_vector[i]) if i in user_vector.indices else 0.0,) for i in range(user_vector.size)]\n",
        "\n",
        "    window_spec = Window.orderBy(F.lit(1))\n",
        "\n",
        "    # Create datafame from data above and inner join with the utility_matrix dataframe\n",
        "    um = spark.createDataFrame(data, ['target']) \\\n",
        "              .withColumn('key', F.row_number().over(window_spec)) \\\n",
        "              .join(self.utility_matrix.withColumn('key', F.row_number().over(window_spec)), on='key', how='inner') \\\n",
        "              .drop('key')\n",
        "\n",
        "    um = um.select('item', 'target', *um.columns[2:]) # Re-arrange the columns\n",
        "\n",
        "    # Merge user columns (include 'target') into a vector column\n",
        "    um = VectorAssembler(inputCols=um.columns[1:],\n",
        "                         outputCol='features').transform(um)\n",
        "\n",
        "    # Find correlation matrix of the 'features' column\n",
        "    pearson = Correlation.corr(um, 'features', 'pearson').head()[0]\n",
        "    correlation_matrix = pearson.toArray()\n",
        "    coefficients = correlation_matrix[0, 1:] # Take the coefficients between 'target' and each user\n",
        "    coefficients = [float(coefficient) for coefficient in coefficients]\n",
        "\n",
        "    pearson_data = [(index, coefficient) for index, coefficient in zip([i for i in range(1, len(um.columns[1:]))], coefficients)]\n",
        "\n",
        "    # Sort coefficient in descending order\n",
        "    pearson_data = sorted(pearson_data,\n",
        "                          key=lambda x: x[1],\n",
        "                          reverse=True)\n",
        "\n",
        "    # If the input user is exists already, exclude it and take first N similar users\n",
        "    pearson_data = pearson_data[1:][:self.N] if pearson_data[0][1] >= 1 else pearson_data[:self.N]\n",
        "\n",
        "    # Sort by index to create SparseVector\n",
        "    pearson_data = sorted(pearson_data,\n",
        "                          key=lambda x: x[0])\n",
        "\n",
        "    # Exclude 'item' and 'features' columns\n",
        "    pearson_sparse_vector = SparseVector(len(um.columns[1:]) - 1, pearson_data)\n",
        "\n",
        "    sum_of_coefficients = float(pearson_sparse_vector.norm(1))\n",
        "\n",
        "    def predict_value(features):\n",
        "      return float(features.dot(pearson_sparse_vector)) / sum_of_coefficients\n",
        "\n",
        "    predict_udf = F.udf(predict_value, DoubleType())\n",
        "\n",
        "    res_df = um.withColumn('predictions', predict_udf(F.col('features'))) \\\n",
        "                .select('item', 'predictions') \\\n",
        "                .sort('predictions', ascending=False) \\\n",
        "                .limit(k)\n",
        "\n",
        "    return res_df"
      ],
      "metadata": {
        "id": "RtSx_S6Z8bYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "x44bE3Z1zX5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(size=467, num_non_zero=25):\n",
        "  # List of possible values from 0.5 to 5.0 with a step of 0.5\n",
        "  possible_values = [round(i * 0.5, 1) for i in range(1, 11)]  # [0.5, 1.0, ..., 5.0]\n",
        "\n",
        "  # Randomly select indices for non-zero entries\n",
        "  non_zero_indices = random.sample(range(size), num_non_zero)\n",
        "\n",
        "  # Assign random values to these indices from the possible_values list\n",
        "  non_zero_values = [random.choice(possible_values) for _ in range(num_non_zero)]\n",
        "\n",
        "  # Create a dictionary for SparseVector\n",
        "  index_value_dict = {index: value for index, value in zip(non_zero_indices, non_zero_values)}\n",
        "\n",
        "  # Create the SparseVector\n",
        "  user_vector = SparseVector(size, index_value_dict)\n",
        "\n",
        "  return user_vector"
      ],
      "metadata": {
        "id": "ozUpbRF0J3NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_vector = generate()\n",
        "print(user_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbJaMUzoVyu2",
        "outputId": "11d8935c-7e2c-468d-d1b8-4d43b35b1fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(467,[6,17,25,37,75,82,90,96,107,121,159,186,253,265,317,319,322,329,360,368,376,380,421,449,466],[4.0,3.5,1.0,3.5,2.0,3.0,1.0,5.0,1.5,2.0,3.0,1.5,5.0,1.0,2.5,4.5,4.5,5.0,5.0,3.0,3.0,3.0,2.5,1.5,1.5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cf = CollaborativeFiltering(5, df)"
      ],
      "metadata": {
        "id": "0aL_WSReG1Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cf.predict(user_vector, 8).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpcSdqBF01YE",
        "outputId": "f22d2b7c-9b2f-46a8-9b6a-57f4f7764088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------------------+\n",
            "|item|       predictions|\n",
            "+----+------------------+\n",
            "| 144| 3.178234998692067|\n",
            "| 199| 3.107416922111042|\n",
            "| 288| 2.867221354370774|\n",
            "| 322|2.8243941327437434|\n",
            "| 176| 2.604340838725143|\n",
            "| 216|2.3222602467952767|\n",
            "|  36| 2.135240187352537|\n",
            "| 440|1.9354102995159885|\n",
            "+----+------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}